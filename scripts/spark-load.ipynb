{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/10 16:10:33 WARN Utils: Your hostname, ngocthang-virtual-machine resolves to a loopback address: 127.0.1.1; using 192.168.29.130 instead (on interface ens33)\n",
      "24/06/10 16:10:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/10 16:10:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.29.130:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>sales-analytics</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7705d295e590>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"sales-analytics\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_directory = 'hdfs://localhost:9000/sales-analytics-data/completely-transformed-data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "dimCountry = spark.read.parquet(f\"{hdfs_directory}dimCountry.parquet\")\n",
    "dimCustomer = spark.read.parquet(f\"{hdfs_directory}dimCustomer.parquet\")\n",
    "dimCustomerStatus = spark.read.parquet(f\"{hdfs_directory}dimCustomerStatus.parquet\")\n",
    "dimDate = spark.read.parquet(f\"{hdfs_directory}dimDate.parquet\")\n",
    "dimProduct = spark.read.parquet(f\"{hdfs_directory}dimProduct.parquet\")\n",
    "dimSupplier = spark.read.parquet(f\"{hdfs_directory}dimSupplier.parquet\")\n",
    "factOrder = spark.read.parquet(f\"{hdfs_directory}factOrder.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|countryID|countryName|\n",
      "+---------+-----------+\n",
      "|       NO|     Norway|\n",
      "|       SE|     Sweden|\n",
      "|       DE|    Germany|\n",
      "+---------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dimCountry.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|customerID|\n",
      "+----------+\n",
      "|     74605|\n",
      "|     40740|\n",
      "|     90143|\n",
      "+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dimCustomer.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------+-------------+----------+------------+----------------+------------+\n",
      "|  orderID|orderPrice|costPerProduct|orderQuantity| orderDate|deliveryDate|customerStatusID|   productID|\n",
      "+---------+----------+--------------+-------------+----------+------------+----------------+------------+\n",
      "|123009195|     226.2|          58.9|            2|2017-01-02|  2017-01-02|           31768|230100100013|\n",
      "|123013358|      41.2|          20.7|            1|2017-01-03|  2017-01-03|           16661|220100100036|\n",
      "|123024632|      34.8|          14.9|            1|2017-01-05|  2017-01-05|           13138|210200600084|\n",
      "+---------+----------+--------------+-------------+----------+------------+----------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "factOrder.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgresql://postgres:12345678@localhost/sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load to postgreSQL\n",
    "def load_data(df, table_name, engine):\n",
    "    df.to_sql(f'{table_name}', engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load succesfully\n"
     ]
    }
   ],
   "source": [
    "# load data to postgeSQL\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgresql://postgres:12345678@localhost/sales') # set up engine\n",
    "# generate a dictionary for a loop with keys are dataframes and value are name of tables\n",
    "dictionary = {\n",
    "    dimCountry:\"dimCountry\",\n",
    "    dimCustomer:\"dimCustomer\",\n",
    "    dimCustomerStatus:\"dimCustomerSattus\",\n",
    "    dimDate:\"dimDate\",\n",
    "    dimProduct:\"dimProduct\",\n",
    "    dimSupplier:\"dimSupplier\",\n",
    "    factOrder:\"factOrder\"\n",
    "}\n",
    "# iterate \n",
    "for df, table_name in dictionary.items():\n",
    "    load_data(df.toPandas(), table_name, engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
